---
layout: archive
title: "Research"
permalink: /Research/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Integrate Multimodal Input to Offer Tailored Feedback In Robotic Tutoring
======
Robotic tutoring systems, as they stand today, exhibit a narrowed spectrum of interaction modalities. Predominantly, they deploy computer vision techniques to oversee student tasks or lean heavily on touch-based interfaces like buttons and touchscreens for direct interactions. Such restricted avenues can sometimes impede the robot's ability to fully discern a student's understanding, consequently affecting the quality of feedback dispensed.

My research, under the guidance of Nicole Salomons, revolves around a compelling proposition: the incorporation of verbal inputs into robotic feedback systems. Through an integraton of visual and verbal data inputs—capturing both the tactile engagements and the vocal expressions of students—we seek to develop a more nuanced model of student understanding. This dual-faceted approach harnesses the power of the LLM model to interpret verbal inputs, thereby enabling robots to provide feedback that is both timely and apt, enriching the overall learning experience.

Develop a Novel AAC Text Generation System Powered by Image Recognition and LLM
======
Communication remains a fundamental challenge for individuals with motor disabilities. Historically, they've had to rely on traditional augmentative and alternative communication (AAC) systems, which come with inherent limitations: symbol-based AAC confines users to a restricted vocabulary, and text-entry methods are painstakingly slow, hampering fluid conversations.

Recognizing these challenges, we developed ImageTalk. Our approach diverges from the norm by seamlessly integrating image recognition models with state-of-the-art large language models (LLMs). The premise is straightforward yet transformative: by interpreting information from user-selected images combined with minimal text inputs, ImageTalk can rapidly generate nuanced stories that reflect a user's intent and context.

The ImageTalk system has a keystroke savings of 94.4%, much higher than traditional text-entry methods. This efficiency translates to faster, richer, and more meaningful interactions for AAC users. Our research offers insights and design guidelines to further optimize this human-AI collaboration, driving our mission to make communication limitations a relic of the past.



More research projects can be found in my [CV](../CV_P_Jiang.pdf), but they are less HCI related.
======
